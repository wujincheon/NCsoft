{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " LDA Latent Dirichlet Allocation\n",
    "\n",
    "특정 토픽에 특정 단어가 나타날 확률을 알려주는 모형. 토픽별로 단어의 분포나 , 문서별로 토픽의 분포 추정 가능.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF = 단어의 수를 세는 방식  \n",
    "\n",
    "문서의 길이에 따라 수를 조정하기도 함.  \n",
    "boolean 빈도로, 등장만 하면 1로 하기도 함.  \n",
    "log로 조정하기도 함.  \n",
    "\n",
    "TF * IDF 값으로. 그 문서에 얼마나 많이 나오는가, 다른 문서에는 얼마나 안나오는 단어인가.  \n",
    "\n",
    "TF는 단어가 문서에 등장하는 횟수로, 같은 단어라도 문서에 따라 다르게 매겨진다.  \n",
    "그러나 DF는 단어가 몇개의 문서에 등장했는지 나타내는 수로, 다른 문서라도 같은 값을 가지게 된다.  \n",
    "IDF는  log(전체 문서수/해당 단어의 DF)를 의미.\n",
    "\n",
    "즉, 전체 문서 뭉치에서 한 단어가 특정 문서에서만 많이 나오면 중요한 단어로 인식하게 하는 것.\n",
    "\n",
    "$$tf-idf = tf * \\log({ N \\over{df}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "the bell이 2007년 등록, 그리고 다양한 신문에서 발생하는 기사들을 얻기 위해 다음 경제뉴스를 이용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "context\n",
    "# 클리닝 함수\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    cleaned_text = re.sub('[a-zA-Z]', ' ', text)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                          ' ', cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정날짜에 해당하는 기사를 크롤링할 링크들을 크롤링."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1='https://media.daum.net/breakingnews/economic?regDate='\n",
    "date=range(20131005,20131012)\n",
    "#20131101 20131108  20131005 20131012    \n",
    "url_2='&page='\n",
    "page= range(1,20)\n",
    "\n",
    "url_list=[]\n",
    "for i in date:\n",
    "    for j in page:\n",
    "        mainurl=url_1 +str(i)+ url_2 + str(j) \n",
    "        source_code_from_URL = urllib.request.urlopen(mainurl, context=context)\n",
    "        soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "        list_news = soup.find('ul', attrs={'class':'list_news2'})\n",
    "        list_news_li = list_news.find_all('li')\n",
    "        for item in list_news_li:\n",
    "            link_txt = item.find('a', attrs={'class':'link_txt'})\n",
    "            url_list.append(link_txt.get('href'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 링크에 대해 단어들을 수집하고 tf idf를 계산하여 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#크롤링1\n",
    "from konlpy.tag import Kkma, Twitter, Komoran\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#twitter=Twitter()\n",
    "kkma=Kkma()\n",
    "\n",
    "\n",
    "mydoclist_kkma=[]\n",
    "\n",
    "\n",
    "for url in url_list:\n",
    "    article=Article(url, language='ko')\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    hoho=article.title+article.text\n",
    "    \n",
    "    kkma_nouns = ' '.join(kkma.nouns(hoho))\n",
    "    mydoclist_kkma.append(kkma_nouns)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "tfidf_vectorizer.fit(mydoclist_kkma)\n",
    "tfidf_matrix_kkma =tfidf_vectorizer.transform(mydoclist_kkma)\n",
    "\n",
    "#document_distances_kkma = tfidf_matrix_kkma * tfidf_matrix_kkma.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내림차순으로 저장해서 tf idf가 높은 것부터 정렬시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "sums = tfidf_matrix_kkma.sum(axis=0)\n",
    "\n",
    "# connecting term to its sums frequency\n",
    "data = []\n",
    "for col, term in enumerate(terms):\n",
    "    data.append( (term, sums[0,col] ))\n",
    "\n",
    "ranking = pd.DataFrame(data, columns=['term','rank'])\n",
    "ranking2=ranking.sort_values('rank', ascending=False)\n",
    "ranking2.to_csv('tfidf.csv', encoding='ms949')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#크롤링2\n",
    "from konlpy.tag import Kkma, Twitter, Komoran\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#twitter=Twitter()\n",
    "kkma=Kkma()\n",
    "\n",
    "\n",
    "mydoclist_kkma2=[]\n",
    "\n",
    "\n",
    "for url in url_list:\n",
    "    article=Article(url, language='ko')\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    hoho=article.title+article.text\n",
    "    \n",
    "    kkma_nouns2 = ' '.join(kkma.nouns(hoho))\n",
    "    mydoclist_kkma2.append(kkma_nouns2)\n",
    "\n",
    "tfidf_vectorizer2 = TfidfVectorizer(min_df=1)\n",
    "tfidf_vectorizer2.fit(mydoclist_kkma2)\n",
    "tfidf_matrix_kkma2 =tfidf_vectorizer2.transform(mydoclist_kkma2)\n",
    "\n",
    "#document_distances_kkma = tfidf_matrix_kkma * tfidf_matrix_kkma.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 금융 상태를 전반적으로 나타내는 단어를 찾기 위해 df값을 이용해서 벡터를 나타내기로 함, 그리고 그 두 벡터를 concate해서 코사인 유사도를 구하고자함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aa={}\n",
    "for i in range(len(list(tfidf_vectorizer2.vocabulary_.keys()))):\n",
    "    aa[list(tfidf_vectorizer2.vocabulary_.keys())[i]]=tfidf_vectorizer2.idf_[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb={}\n",
    "for i in range(len(list(tfidf_vectorizer.vocabulary_.keys()))):\n",
    "    bb[list(tfidf_vectorizer.vocabulary_.keys())[i]]=tfidf_vectorizer.idf_[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc={}\n",
    "dd={}\n",
    "for i in aa.keys():\n",
    "    cc[i]=0\n",
    "    dd[i]=0\n",
    "for j in bb.keys():\n",
    "    cc[j]=0\n",
    "    dd[j]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in aa.keys():\n",
    "    cc[i]=aa[i]\n",
    "for j in bb.keys():\n",
    "    dd[j]=bb[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccc=list(cc.values())\n",
    "ddd=list(dd.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(65326):\n",
    "    ccc[i]=2100/exp(ccc[i]-1)\n",
    "    ddd[i]=2100/exp(ddd[i]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.0025001]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import linalg, mat, dot\n",
    "mc=mat(ccc)\n",
    "md=mat(ddd)\n",
    "sim_cd=dot(mc,md.T) / (linalg.norm(mc) * linalg.norm(md))\n",
    "sim_cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://hero4earth.com/blog/projects/2018/01/21/naver_movie_review/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65326"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "군집 시작일 전 10일간의 기사를 기준으로 분포 확인."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "doc2vec = 단어의 벡터 거리를 기준으로 유사도를 확인하는 방식\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토픽 모델링을 이용한 뉴스기사 품질 평가 기사에대한 meta data 형성 후, 기사를 군집화 분류 / 뉴스 기사 제목으로 토픽 모델링. / 토픽 단어 분포와, 기사의 토픽 분포로 연관성score 도출 / 주어진 키워드에 대해 토픽 분포와 이질적으로 나타나는 기사를 선정(고유성 높음) - PCA기반, 가우시안 밀도 추정, knn, local outlier factors "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
